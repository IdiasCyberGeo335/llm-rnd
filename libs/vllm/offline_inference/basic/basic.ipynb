{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74645989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 22:03:43 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d7f9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What songs do you know? What you would recomend?\"\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    n=1,\n",
    "    # best_of=3,\n",
    "    presence_penalty=0.1,\n",
    "    frequency_penalty=0.1,\n",
    "    repetition_penalty=1,    \n",
    "    temperature=1,\n",
    "    # top_p=0.85,\n",
    "    # top_k=30,\n",
    "    # min_p=0.7,\n",
    "    seed=42,\n",
    "    # stop=\n",
    "    # stop_token_ids=\n",
    "    # bad_words=\n",
    "    # include_stop_str_in_output=\n",
    "    # ignore_eos=\n",
    "    max_tokens=512 \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4edbd885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 22:03:47 [config.py:2968] Downcasting torch.float32 to torch.float16.\n",
      "INFO 06-13 22:03:53 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-13 22:03:53 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 06-13 22:03:54 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='openai-community/gpt2', speculative_config=None, tokenizer='openai-community/gpt2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=openai-community/gpt2, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 06-13 22:03:54 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79ec304dc860>\n",
      "INFO 06-13 22:03:55 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-13 22:03:55 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 06-13 22:03:55 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-13 22:03:55 [gpu_model_runner.py:1329] Starting to load model openai-community/gpt2...\n",
      "INFO 06-13 22:03:55 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 06-13 22:03:55 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfc650bb797437cb82453672fb10674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 22:03:56 [loader.py:458] Loading weights took 0.15 seconds\n",
      "INFO 06-13 22:03:56 [gpu_model_runner.py:1347] Model loading took 0.2378 GiB and 0.861255 seconds\n",
      "INFO 06-13 22:03:57 [backends.py:420] Using cache directory: /home/retro0/.cache/vllm/torch_compile_cache/d52fc69172/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-13 22:03:57 [backends.py:430] Dynamo bytecode transform time: 1.44 s\n",
      "INFO 06-13 22:03:58 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 0.555 s\n",
      "INFO 06-13 22:03:58 [monitor.py:33] torch.compile takes 1.44 s in total\n",
      "INFO 06-13 22:03:59 [kv_cache_utils.py:634] GPU KV cache size: 267,888 tokens\n",
      "INFO 06-13 22:03:59 [kv_cache_utils.py:637] Maximum concurrency for 1,024 tokens per request: 261.61x\n",
      "INFO 06-13 22:04:14 [gpu_model_runner.py:1686] Graph capturing finished in 15 secs, took 0.17 GiB\n",
      "INFO 06-13 22:04:14 [core.py:159] init engine (profile, create kv cache, warmup model) took 18.26 seconds\n",
      "INFO 06-13 22:04:14 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc67f331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c000a3c4f0449ba516da36c751526e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8447c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: What songs do you know? What you would recomend?\n",
      "generated_text:  What you would share?\n",
      "\n",
      "Duncan Wheaton: When I was growing up, I drew with my 6-year-old, and wondered for a mere minute what my class holds in store for me. I perceived myself as an open-minded and strongman artist -- and I scrutinized situations. Guided Ty like a Kim Stanley Robinson comic story at 20, then sculpted in many different ways from released albums and television adverts. Every impression matters greatly - or, as Keith says, \"I'm quite sure that putting a few stories together is what made my style so famous in the first place.\" But illustrators should be trained well to incorporate own creations into their writing process. When competing for Columnists and Writers Weekly's anthology awards, Robert Ginsberg revealed he pulled about 87 episodes of \"Dead Mansion\" on a night out, poking fun at a deeply resentful colleague; there are some sympathetic finales here but really vocationally arousing to listen to whenever he makes his drivel sure to draw in any male reader wanting someone to read more harshly. Also: Understand that solo artists are not your idol or dÃ©but album after somebody sings, \"Andgive God.\"\n",
      "\n",
      "Music is discovered by people who have already found it and challenged it in a healthy way. I'm just that important.\n",
      "\n",
      "Do you eat takeout every morning now?\n",
      "\n",
      "UK Classics is rock n roll's choice of pop artists, with scores of considered all-star starters including Ronald Reagan and Dick Tracy. This column began ... and I'm still working on \"Madagascar Rock\" to boot...morenoWaffe\n",
      "\n",
      "What do bookazines cover sometimes 2013?\n",
      "\n",
      "Veronica Watson: A small number have covered the press saints once again defying the pressure of regular newspapers from trade-entry to public service as part of taking part in the Star Awards, which disrespects that act of independence by receiving only 10 percent of their permanent readership. The 2009 faded London Times bestseller prize cover was packed with accolades, both literary and indie. Lee Turner again last year had the last words in the defence of women on film as secured pro bono by Time.\n",
      "\n",
      "What does it mean to be adrift? What we mean by it all is that we are seen and bastardized when we leave the indentured servitude we find ourselves trapped in international poverty of ending up at sea level worse than Haiti. We get accepted into our beds, and consigned to bedlock extraordinas as pets, slobs and\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"generated_text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef1b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c965859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "back_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
